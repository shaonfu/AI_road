#%%
"""
åœ¨TensorFlowä¸­,è¦å®ç°å…¨è¿æ¥å±‚ï¼Œ
åªéœ€è¦å®šä¹‰å¥½æƒå€¼å¼ é‡Wå’Œåç½®å¼ é‡bï¼Œ
å¹¶åˆ©ç”¨TensorFlowæä¾›çš„æ‰¹é‡çŸ©é˜µç›¸ä¹˜å‡½æ•°
tf.matmul()å³å¯å®Œæˆç½‘ç»œå±‚çš„è®¡ç®—ã€‚
"""
"""
å¦‚ä¸‹ä»£ç åˆ›å»ºè¾“å…¥XçŸ©é˜µä¸ºğ‘ = 2ä¸ªæ ·æœ¬ï¼Œ
æ¯ä¸ªæ ·æœ¬çš„è¾“å…¥ç‰¹å¾é•¿åº¦ä¸ºğ‘‘ğ‘–ğ‘› = 784ï¼Œ
è¾“å‡ºèŠ‚ç‚¹æ•°ä¸ºğ‘‘ğ‘œğ‘¢ğ‘¡ = 256ï¼Œæ•…å®šä¹‰æƒå€¼çŸ©é˜µWçš„shape 
ä¸º[784,256]ï¼Œå¹¶é‡‡ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–W;
åç½®å‘é‡ b çš„ shape å®šä¹‰ä¸º[256]ï¼Œ
åœ¨è®¡ç®—å®ŒX@Wåç›¸åŠ å³å¯ï¼Œæœ€ç»ˆå…¨è¿æ¥å±‚çš„è¾“å‡ºOçš„shape
ä¸º [2,256]ï¼Œå³ 2 ä¸ªæ ·æœ¬çš„ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾é•¿åº¦ä¸º 256ã€‚
"""
import tensorflow as tf

x = tf.random.normal([2,784])
w1 = tf.Variable(tf.random.truncated_normal([784,256],stddev=0.1))
b1 = tf.Variable(tf.zeros([256]))
o1 = tf.matmul(x,w1) + b1 #çº¿æ€§å˜æ¢
o1 = tf.nn.relu(o1)   #æ¿€æ´»å‡½æ•°

#%%
#å±‚æ–¹å¼å®ç° é«˜å±‚api
x = tf.random.normal([4,28*28])
from tensorflow.keras import layers #å¯¼å…¥å±‚æ¨¡å—
#åˆ›å»ºå…¨è¿æ¥å±‚,æŒ‡å®šè¾“å‡ºèŠ‚ç‚¹æ•°å’Œæ¿€æ´»å‡½æ•°
fc = layers.Dense(512,activation=tf.nn.relu)
h1 = fc(x)   #é€šè¿‡fcç±»å®Œæˆä¸€æ¬¡å…¨è¿æ¥å±‚çš„è¿ç®—

# %%
#é€šè¿‡ç±»å†…éƒ¨çš„æˆå‘˜åkernelå’Œbiasæ¥è·å–æƒå€¼çŸ©é˜µWå’Œåç½®b
print(fc.kernel)
print(fc.bias)

# %%
"""
ç¥ç»ç½‘ç»œ
é€šè¿‡å †å 4ä¸ªå…¨è¿æ¥å±‚ï¼Œå¯ä»¥è·å¾—å±‚æ•°ä¸º4çš„ç¥ç»ç½‘ç»œï¼Œ
ç”±äºæ¯å±‚å‡ä¸ºå…¨è¿æ¥å±‚ï¼Œç§°ä¸ºå…¨è¿æ¥ç½‘ç»œ
"""
"""
åœ¨è®¾è®¡å…¨è¿æ¥ç½‘ç»œæ—¶ï¼Œç½‘ç»œçš„ç»“æ„é…ç½®ç­‰è¶…å‚æ•°
å¯ä»¥æŒ‰ç€ç»éªŒæ³•åˆ™è‡ªç”±è®¾ç½®ï¼Œåªéœ€è¦éµå¾ªå°‘é‡çš„çº¦æŸå³å¯ã€‚
å…¶ä¸­éšè—å±‚1çš„è¾“å…¥èŠ‚ç‚¹æ•°éœ€å’Œæ•°æ®çš„å®é™…ç‰¹å¾é•¿åº¦åŒ¹é…ï¼Œ
æ¯å±‚çš„è¾“å…¥å±‚èŠ‚ç‚¹æ•°ä¸ä¸Šä¸€å±‚è¾“å‡ºèŠ‚ç‚¹æ•°åŒ¹é…ï¼Œ
è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°å’ŒèŠ‚ç‚¹æ•°éœ€è¦æ ¹æ®ä»»åŠ¡çš„å…·ä½“è®¾å®šè¿›è¡Œè®¾è®¡ã€‚
æ€»çš„æ¥è¯´ï¼Œç¥ç»ç½‘ç»œç»“æ„çš„è‡ªç”±åº¦è¾ƒå¤§.
"""
#å¼ é‡æ–¹å¼å®ç°
#éšè—å±‚1å¼ é‡
w1 = tf.Variable(tf.random.truncated_normal([784,256],stddev=0.1))
b1 = tf.Variable(tf.zeros([256]))
#éšè—å±‚2å¼ é‡
w2 = tf.Variable(tf.random.truncated_normal([256,128],stddev=0.1))
b2 = tf.Variable(tf.zeros([128]))
#éšè—å±‚3
w3 = tf.Variable(tf.random.truncated_normal([128,64],stddev=0.1))
b3 = tf.Variable(tf.zeros([64]))
#è¾“å‡ºå±‚å¼ é‡
w4 = tf.Variable(tf.random.truncated_normal([64,10],stddev=0.1))
b4 = tf.Variable(tf.zeros([10]))

with tf.GradientTape() as tape:   #æ¢¯åº¦è®°å½•å™¨
    # x: [b,28*28]
    # éšè—å±‚1å‰å‘è®¡ç®—,[b,28*28]=>[b,256]
    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])
    h1 = tf.nn.relu(h1)
    # éšè—å±‚2å‰å‘è®¡ç®—,[b,256] => [b,128]
    h2 = h1@w2 + b2
    h2 = tf.nn.relu(h2)
    # éšè—å±‚ 3 å‰å‘è®¡ç®—ï¼Œ[b, 128] => [b, 64] h3 = h2@w3 + b3
    h3 = tf.nn.relu(h3)
    # è¾“å‡ºå±‚å‰å‘è®¡ç®—ï¼Œ[b, 64] => [b, 10] h4 = h3@w4 + b4
    h4 = h3@w4 + b4


#%%
#å±‚æ–¹å¼å®ç°
#æ–°å»ºå„ä¸ªç½‘ç»œå±‚,å¹¶æŒ‡å®šå„å±‚çš„æ¿€æ´»å‡½æ•°ç±»å‹
fc1 = layers.Dense(256,activation=tf.nn.relu)
fc2 = layers.Dense(128,activation=tf.nn.relu)
fc3 = layers.Dense(64,activation=tf.nn.relu)
fc4 = layers.Dense(10,activation=None)  #è¾“å‡ºå±‚

#åœ¨å‰å‘è®¡ç®—æ—¶,ä¾åºé€šè¿‡å„ä¸ªç½‘ç»œå±‚å³å¯:
x = tf.random.normal([4,28*28])
h1 = fc1(x)
h2 = fc2(h1)
h3 = fc3(h2)
h4 = fc4(h3)

#é€šè¿‡Sequentialå®¹å™¨å°è£…ä¸ºä¸€ä¸ªç½‘ç»œç±»
model = layers.Sequential([
    layers.Dense(256,activation=tf.nn.relu),
    layers.Dense(128,activation=tf.nn.relu),
    layers.Dense(64,activation=tf.nn.relu),
    layers.Dense(10,activation=None)
])

#å‰å‘è®¡ç®—æ—¶åªéœ€è¦è°ƒç”¨ä¸€æ¬¡ç½‘ç»œå¤§ç±»å¯¹è±¡å³å¯å®Œæˆæ‰€æœ‰å±‚çš„æŒ‰åºè®¡ç®—:
out = model(x)

#%%
#è¾“å‡ºå±‚è®¾è®¡
#[0,1]åŒºé—´,å’Œä¸º1
#softmaxå‡½æ•°ä¸ä»…å¯ä»¥å°†è¾“å‡ºå€¼æ˜ å°„åˆ°[0,1]åŒºé—´
#è¿˜æ»¡è¶³æ‰€æœ‰çš„è¾“å‡ºå€¼ä¹‹å’Œä¸º1çš„ç‰¹æ€§
"""
é¿å…å•ç‹¬ä½¿ç”¨Softmaxå‡½æ•°ä¸äº¤å‰ç†µæŸå¤±å‡½æ•°
ä¸‹å‡½æ•°å°†Softmaxä¸äº¤å‰ç†µæŸå¤±å‡½æ•°åŒæ—¶å®ç°
å‡½æ•°å¼æ¥å£ä¸ºtf.keras.losses.categorical_crossentropy(y_true, y_pred,from_logits=False)
å…¶ä¸­y_trueä»£è¡¨äº† one-hot ç¼–ç åçš„çœŸå®æ ‡ç­¾,
y_predè¡¨ç¤ºç½‘ç»œçš„é¢„æµ‹å€¼,å½“from_logitsè®¾ç½®ä¸ºTrueæ—¶,
y_predè¡¨ç¤ºé¡»ä¸ºæœªç»è¿‡Softmaxå‡½æ•°çš„å˜é‡z;
å½“from_logitsè®¾ç½®ä¸ºFalseæ—¶,y_predè¡¨ç¤ºä¸ºç»è¿‡Softmaxå‡½æ•°çš„è¾“å‡ºã€‚
"""
z = tf.constant([2.,1.,0.1])
tf.nn.softmax(z)

# %%
z = tf.random.normal([2,10])    #æ„é€ è¾“å‡ºå±‚çš„è¾“å‡º
y_onehot = tf.constant([1,3])   #æ„é€ çœŸå®å€¼
y_onehot = tf.one_hot(y_onehot,depth=10)  #one-hotç¼–ç 
#è¾“å‡ºå±‚æœªä½¿ç”¨Softmaxå‡½æ•°,æ•…from_logitsè®¾ç½®ä¸ºTrue
loss = tf.keras.losses.categorical_crossentropy(y_onehot,z,from_logits=True)
loss = tf.reduce_mean(loss)   #è®¡ç®—å¹³å‡äº¤å‰ç†µæŸå¤±
loss

# %%
"""
ä¹Ÿå¯ä»¥åˆ©ç”¨losses.CategoricalCrossentropy(from_logits)ç±»
æ–¹å¼åŒæ—¶å®ç°Softmaxä¸äº¤å‰ç†µæŸå¤±å‡½æ•°çš„è®¡ç®—:
"""
#åˆ›å»ºSoftmaxä¸äº¤å‰ç†µè®¡ç®—ç±»,è¾“å‡ºå±‚çš„è¾“å‡ºzæœªä½¿ç”¨softmax
criteon = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
loss = criteon(y_onehot,z)
loss

# %%
#è¯¯å·®è®¡ç®—
"""
å¸¸è§çš„è¯¯å·®è®¡ç®—å‡½æ•°æœ‰å‡æ–¹å·®ã€äº¤å‰ç†µã€KLæ•£åº¦ã€
Hinge Losså‡½æ•°ç­‰,å…¶ä¸­å‡æ–¹å·®å‡½æ•°å’Œäº¤å‰ç†µå‡½æ•°åœ¨
æ·±åº¦å­¦ä¹ ä¸­æ¯”è¾ƒå¸¸è§ï¼Œå‡æ–¹å·®ä¸»è¦ç”¨äºå›å½’é—®é¢˜ï¼Œäº¤å‰ç†µä¸»è¦ç”¨äºåˆ†ç±»é—®é¢˜ã€‚
"""
#å‡æ–¹å·®
#å‡æ–¹å·®è¯¯å·®(Mean Squared Error, MSE)å‡½æ•°
#æŠŠè¾“å‡ºå‘é‡å’ŒçœŸå®å‘é‡æ˜ å°„åˆ°ç¬›å¡å°”åæ ‡ç³»
